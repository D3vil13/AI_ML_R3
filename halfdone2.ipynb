{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError, CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " import necessary libraries and modules such as NumPy, Pandas, TensorFlow, Keras, and specific Keras components needed for building and training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:\\\\Users\\\\d3vsh\\\\Downloads\\\\Rock Paper Scissors SXSW.v12i.tensorflow\\\\train\\\\_annotations.csv')\n",
    "valid_data = pd.read_csv('C:\\\\Users\\\\d3vsh\\\\Downloads\\\\Rock Paper Scissors SXSW.v12i.tensorflow\\\\valid\\\\_annotations.csv')\n",
    "test_data = pd.read_csv('C:\\\\Users\\\\d3vsh\\\\Downloads\\\\Rock Paper Scissors SXSW.v12i.tensorflow\\\\test\\\\_annotations.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read CSV files containing data for training, validation, and testing from specified file paths into Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width, image_height = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the dimensions (width and height) to which the input images will be resized during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {'Rock': 0, 'Paper': 1, 'Scissors': 2}\n",
    "\n",
    "\n",
    "train_data = train_data.dropna(subset=['label'])\n",
    "valid_data = valid_data.dropna(subset=['label'])\n",
    "test_data = test_data.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dictionary to map class labels ('Rock', 'Paper', 'Scissors') to numerical labels (0, 1, 2) for classification.\n",
    "python\n",
    "Removes rows with missing 'label' values from the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_classification = [class_labels[label] for label in train_data['label']]\n",
    "y_valid_classification = [class_labels[label] for label in valid_data['label']]\n",
    "y_test_classification = [class_labels[label] for label in test_data['label']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts the class labels in the data to their corresponding numerical labels using the class_labels dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data):\n",
    "    images = []\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        image_path = os.path.join('C:\\\\Users\\\\d3vsh\\\\Downloads\\\\Rock Paper Scissors SXSW.v12i.tensorflow', str(row['filename']))\n",
    "        if os.path.exists(image_path):\n",
    "            image = keras.preprocessing.image.load_img(image_path, target_size=(image_width, image_height))\n",
    "            image = keras.preprocessing.image.img_to_array(image)\n",
    "            image /= 255.0\n",
    "            images.append(image)\n",
    "\n",
    "            bounding_box = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "            bounding_boxes.append(bounding_box)\n",
    "\n",
    "    return np.array(images), np.array(bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a function load_and_preprocess_data that takes a DataFrame data as input and processes the images and bounding boxes associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_bounding_box = load_and_preprocess_data(train_data)\n",
    "X_valid, y_valid_bounding_box = load_and_preprocess_data(valid_data)\n",
    "X_test, y_test_bounding_box = load_and_preprocess_data(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls the load_and_preprocess_data function to load and preprocess images and bounding boxes for training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, y_classification, y_bounding_box, batch_size):\n",
    "        self.X = X\n",
    "        self.y_classification = y_classification\n",
    "        self.y_bounding_box = y_bounding_box\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_X = self.X[start:end]\n",
    "        batch_classification = self.y_classification[start:end]\n",
    "        batch_bounding_box = self.y_bounding_box[start:end]\n",
    "        return batch_X, {'classification': batch_classification, 'bounding_box': batch_bounding_box}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a custom data generator class DataGenerator that inherits from keras.utils.Sequence. This class is used to generate batches of data for training, where each batch contains both images and their corresponding classification and bounding box labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "    maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    \n",
    "    flatten = Flatten()(maxpool2)\n",
    "    dense1 = Dense(128, activation='relu')(flatten)\n",
    "\n",
    "    #\n",
    "    bounding_box_output = Dense(4, activation='linear', name='bounding_box')(dense1)\n",
    "\n",
    "   \n",
    "    classification_output = Dense(num_classes, activation='softmax', name='classification')(dense1)\n",
    "   \n",
    "    model = Model(inputs=input_layer, outputs=[classification_output, bounding_box_output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a function custom_model that constructs a custom neural network model using Keras. The model has two output branches, one for classification and another for bounding box regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (image_width, image_height, 3)\n",
    "num_classes = len(class_labels)\n",
    "model = custom_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifies the input shape (image dimensions) and the number of classes and constructs the custom model using the custom_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'classification': CategoricalCrossentropy(), 'bounding_box': MeanSquaredError()},\n",
    "              metrics={'classification': 'accuracy', 'bounding_box': 'mae'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiles the model with the Adam optimizer, two different loss functions (CategoricalCrossentropy for classification and MeanSquaredError for bounding box regression), and metrics for monitoring model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efines batch size and the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(X_train, y_train_classification, y_train_bounding_box, batch_size)\n",
    "valid_data_generator = DataGenerator(X_valid, y_valid_classification, y_valid_bounding_box, batch_size)\n",
    "test_data_generator = DataGenerator(X_test, y_test_classification, y_test_bounding_box, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates instances of the DataGenerator class for training, validation, and test data, which will be used to generate batches of data during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
