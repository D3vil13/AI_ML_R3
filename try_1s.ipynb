{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " import the necessary libraries and modules. TensorFlow is used for deep learning, and Keras is a high-level neural networks API running on top of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/d3vsh/Downloads/train_main'\n",
    "valid_dir = 'C:\\\\Users\\\\d3vsh\\\\Downloads\\\\valid_main'\n",
    "\n",
    "test_dir = 'C:/Users/d3vsh/Downloads/test_main'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the paths to the directories containing the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0 / 255.0,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create data generators for training, validation, and testing. They are used to preprocess and augment the image data. train_datagen includes various image transformations like rotation, shifting, shearing, zooming, and horizontal flipping. valid_datagen and test_datagen only rescale the images by dividing pixel values by 255 to normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "\n",
    "target_size = (64, 64)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7134 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 385 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines use the data generators to create iterators for the training, validation, and test datasets. They load images from the specified directories, resize them to target_size, and convert class labels to one-hot encoded categorical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Sequential model, a linear stack of layers. It starts with a series of Conv2D layers (convolutional layers) followed by MaxPooling2D layers (pooling layers) to extract features from the input images. The Conv2D layers use ReLU activation functions. After the convolutional layers, there's a Flatten layer to convert the 2D feature maps into a 1D vector, followed by fully connected Dense layers. The final layer has three units with a softmax activation function for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.002), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compiles the model, specifying the optimizer (Adam with a learning rate of 0.002), the loss function (categorical cross-entropy), and metrics to monitor during training (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "223/223 [==============================] - 34s 86ms/step - loss: 1.0796 - accuracy: 0.4127 - val_loss: 0.9779 - val_accuracy: 0.5403\n",
      "Epoch 2/40\n",
      "223/223 [==============================] - 19s 86ms/step - loss: 1.0188 - accuracy: 0.4821 - val_loss: 0.9359 - val_accuracy: 0.5247\n",
      "Epoch 3/40\n",
      "223/223 [==============================] - 19s 87ms/step - loss: 0.9559 - accuracy: 0.5336 - val_loss: 0.8026 - val_accuracy: 0.6078\n",
      "Epoch 4/40\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.8916 - accuracy: 0.5704 - val_loss: 0.7175 - val_accuracy: 0.6442\n",
      "Epoch 5/40\n",
      "223/223 [==============================] - 20s 87ms/step - loss: 0.8476 - accuracy: 0.6105 - val_loss: 0.6381 - val_accuracy: 0.7247\n",
      "Epoch 6/40\n",
      "223/223 [==============================] - 18s 83ms/step - loss: 0.8150 - accuracy: 0.6353 - val_loss: 0.5730 - val_accuracy: 0.7506\n",
      "Epoch 7/40\n",
      "223/223 [==============================] - 18s 79ms/step - loss: 0.7907 - accuracy: 0.6482 - val_loss: 0.5558 - val_accuracy: 0.7792\n",
      "Epoch 8/40\n",
      "223/223 [==============================] - 20s 89ms/step - loss: 0.7681 - accuracy: 0.6679 - val_loss: 0.5123 - val_accuracy: 0.8078\n",
      "Epoch 9/40\n",
      "223/223 [==============================] - 19s 86ms/step - loss: 0.7233 - accuracy: 0.6947 - val_loss: 0.5224 - val_accuracy: 0.8026\n",
      "Epoch 10/40\n",
      "223/223 [==============================] - 21s 93ms/step - loss: 0.7211 - accuracy: 0.6909 - val_loss: 0.4890 - val_accuracy: 0.8000\n",
      "Epoch 11/40\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.6997 - accuracy: 0.7030 - val_loss: 0.5495 - val_accuracy: 0.7429\n",
      "Epoch 12/40\n",
      "223/223 [==============================] - 20s 90ms/step - loss: 0.6789 - accuracy: 0.7087 - val_loss: 0.5206 - val_accuracy: 0.7818\n",
      "Epoch 13/40\n",
      "223/223 [==============================] - 20s 87ms/step - loss: 0.6745 - accuracy: 0.7146 - val_loss: 0.4539 - val_accuracy: 0.8182\n",
      "Epoch 14/40\n",
      "223/223 [==============================] - 19s 83ms/step - loss: 0.6662 - accuracy: 0.7174 - val_loss: 0.4365 - val_accuracy: 0.8208\n",
      "Epoch 15/40\n",
      "223/223 [==============================] - 9s 40ms/step - loss: 0.6422 - accuracy: 0.7335 - val_loss: 0.4100 - val_accuracy: 0.8260\n",
      "Epoch 16/40\n",
      "223/223 [==============================] - 9s 42ms/step - loss: 0.6457 - accuracy: 0.7253 - val_loss: 0.4051 - val_accuracy: 0.8519\n",
      "Epoch 17/40\n",
      "223/223 [==============================] - 9s 41ms/step - loss: 0.6369 - accuracy: 0.7299 - val_loss: 0.4241 - val_accuracy: 0.8312\n",
      "Epoch 18/40\n",
      "223/223 [==============================] - 10s 46ms/step - loss: 0.6224 - accuracy: 0.7425 - val_loss: 0.4072 - val_accuracy: 0.8494\n",
      "Epoch 19/40\n",
      "223/223 [==============================] - 12s 53ms/step - loss: 0.6041 - accuracy: 0.7534 - val_loss: 0.3892 - val_accuracy: 0.8753\n",
      "Epoch 20/40\n",
      "223/223 [==============================] - 11s 50ms/step - loss: 0.5836 - accuracy: 0.7670 - val_loss: 0.3575 - val_accuracy: 0.8753\n",
      "Epoch 21/40\n",
      "223/223 [==============================] - 9s 41ms/step - loss: 0.5698 - accuracy: 0.7708 - val_loss: 0.3706 - val_accuracy: 0.8857\n",
      "Epoch 22/40\n",
      "223/223 [==============================] - 9s 40ms/step - loss: 0.5659 - accuracy: 0.7697 - val_loss: 0.3686 - val_accuracy: 0.8649\n",
      "Epoch 23/40\n",
      "223/223 [==============================] - 9s 40ms/step - loss: 0.5728 - accuracy: 0.7677 - val_loss: 0.3330 - val_accuracy: 0.8805\n",
      "Epoch 24/40\n",
      "223/223 [==============================] - 18s 80ms/step - loss: 0.5425 - accuracy: 0.7834 - val_loss: 0.3368 - val_accuracy: 0.8727\n",
      "Epoch 25/40\n",
      "223/223 [==============================] - 16s 71ms/step - loss: 0.5634 - accuracy: 0.7747 - val_loss: 0.4182 - val_accuracy: 0.8364\n",
      "Epoch 26/40\n",
      "223/223 [==============================] - 9s 39ms/step - loss: 0.5521 - accuracy: 0.7757 - val_loss: 0.3497 - val_accuracy: 0.8831\n",
      "Epoch 27/40\n",
      "223/223 [==============================] - 9s 40ms/step - loss: 0.5609 - accuracy: 0.7714 - val_loss: 0.3625 - val_accuracy: 0.8675\n",
      "Epoch 28/40\n",
      "223/223 [==============================] - 12s 56ms/step - loss: 0.5351 - accuracy: 0.7826 - val_loss: 0.3235 - val_accuracy: 0.8727\n",
      "Epoch 29/40\n",
      "223/223 [==============================] - 13s 58ms/step - loss: 0.5270 - accuracy: 0.7904 - val_loss: 0.3030 - val_accuracy: 0.8935\n",
      "Epoch 30/40\n",
      "223/223 [==============================] - 10s 44ms/step - loss: 0.5402 - accuracy: 0.7822 - val_loss: 0.3580 - val_accuracy: 0.8805\n",
      "Epoch 31/40\n",
      "223/223 [==============================] - 10s 44ms/step - loss: 0.5228 - accuracy: 0.7938 - val_loss: 0.2860 - val_accuracy: 0.8961\n",
      "Epoch 32/40\n",
      "223/223 [==============================] - 9s 39ms/step - loss: 0.5166 - accuracy: 0.7900 - val_loss: 0.3272 - val_accuracy: 0.8649\n",
      "Epoch 33/40\n",
      "223/223 [==============================] - 10s 44ms/step - loss: 0.5178 - accuracy: 0.7937 - val_loss: 0.2895 - val_accuracy: 0.9013\n",
      "Epoch 34/40\n",
      "223/223 [==============================] - 9s 42ms/step - loss: 0.5050 - accuracy: 0.7991 - val_loss: 0.3010 - val_accuracy: 0.8935\n",
      "Epoch 35/40\n",
      "223/223 [==============================] - 9s 40ms/step - loss: 0.5139 - accuracy: 0.7994 - val_loss: 0.2566 - val_accuracy: 0.9091\n",
      "Epoch 36/40\n",
      "223/223 [==============================] - 9s 42ms/step - loss: 0.4969 - accuracy: 0.8011 - val_loss: 0.2687 - val_accuracy: 0.9195\n",
      "Epoch 37/40\n",
      "223/223 [==============================] - 14s 62ms/step - loss: 0.4976 - accuracy: 0.8039 - val_loss: 0.2783 - val_accuracy: 0.8935\n",
      "Epoch 38/40\n",
      "223/223 [==============================] - 9s 41ms/step - loss: 0.5168 - accuracy: 0.7976 - val_loss: 0.3013 - val_accuracy: 0.8961\n",
      "Epoch 39/40\n",
      "223/223 [==============================] - 9s 39ms/step - loss: 0.5088 - accuracy: 0.7998 - val_loss: 0.2805 - val_accuracy: 0.9117\n",
      "Epoch 40/40\n",
      "223/223 [==============================] - 9s 41ms/step - loss: 0.4884 - accuracy: 0.8123 - val_loss: 0.3102 - val_accuracy: 0.8935\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 0.4085 - accuracy: 0.8738\n",
      "Test accuracy: 0.8737863898277283\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=40,\n",
    "    validation_data=valid_generator\n",
    ")\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "model.save('rock_paper_scissors_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains the model using the fit method. It trains for 40 epochs using the training data generator and validates using the validation data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
